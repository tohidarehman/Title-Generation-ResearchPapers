{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iPFNEJwG7N0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for meteor\n",
        "!pip install moverscore\n",
        "!pip install pyemd\n",
        "!pip install pytorch-pretrained-bert"
      ],
      "metadata": {
        "id": "8ex5-G1aa0y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install bert-score\n",
        "!pip install nltk\n",
        "!pip install pandas\n",
        "!pip install tqdm\n",
        "!pip install evaluate\n",
        "!pip install rouge_score\n",
        "#from datasets import load_dataset, load_metric\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from bert_score import BERTScorer\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "I1NEA_8QHqF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5aBogxt63_yX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cd /content/drive/MyDrive/Style-Title-generation-ICADL/LREC-Coling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRW69jvNe_ji",
        "outputId": "c407fa26-18df-4f6a-b473-50dda88edcf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Style-Title-generation-ICADL/LREC-Coling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWy6s8wVsvtl",
        "outputId": "b0a5b43a-1d2d-480b-b156-63d081a75033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Catchy-moverScore.csv\t\t\t   Funny-score-Rouge-Meteor-BERT.csv\n",
            "Catchy-moverScore.txt\t\t\t   Funny-score-Rouge-Meteor-BERT.txt\n",
            "Catchy-SciBERTScore.csv\t\t\t   LREC-Coling-Style-Title.csv\n",
            "Catchy-SciBERTScore.txt\t\t\t   LREC-Coling-Style-Title.xlsx\n",
            "Catchy-score-Rouge-Meteor-BERT.csv\t   Plain-moverScore.csv\n",
            "Catchy-score-Rouge-Meteor-BERT.txt\t   Plain-moverScore.txt\n",
            "Comprehensive-moverScore.csv\t\t   Plain-SciBERTScore.csv\n",
            "Comprehensive-moverScore.txt\t\t   Plain-SciBERTScore.txt\n",
            "Comprehensive-SciBERTScore.csv\t\t   Plain-score-Rouge-Meteor-BERT.csv\n",
            "Comprehensive-SciBERTScore.txt\t\t   Plain-score-Rouge-Meteor-BERT.txt\n",
            "Comprehensive-score-Rouge-Meteor-BERT.csv  Rhyming-moverScore.csv\n",
            "Comprehensive-score-Rouge-Meteor-BERT.txt  Rhyming-moverScore.txt\n",
            "Funny-moverScore.csv\t\t\t   Rhyming-SciBERTScore.csv\n",
            "Funny-moverScore.txt\t\t\t   Rhyming-SciBERTScore.txt\n",
            "Funny-SciBERTScore.csv\t\t\t   Rhyming-score-Rouge-Meteor-BERT.csv\n",
            "Funny-SciBERTScore.txt\t\t\t   Rhyming-score-Rouge-Meteor-BERT.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the Excel file\n",
        "data_final = pd.read_excel(\"LREC-Coling-Style-Title.xlsx\")\n",
        "\n",
        "# Print the total number of rows\n",
        "print(len(data_final))\n",
        "data_final = data_final.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
        "# Save the updated DataFrame back to a CSV file (optional)\n",
        "data_final.to_csv(\"LREC-Coling-Style-Title.csv\", index=False)\n",
        "\n",
        "# Display the updated DataFrame (optional)\n",
        "print(data_final.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UClMDorqHPI",
        "outputId": "a7c98a11-54a6-47c7-a413-6973e26bf818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "                                            Abstract  \\\n",
            "0  Multimodal machine translation (MMT) is a chal...   \n",
            "1  Natural Language Processing (NLP) models tend ...   \n",
            "2  Previous stance detection studies typically co...   \n",
            "3  Empathy is critical for effective communicatio...   \n",
            "4  Sentence representation learning is a fundamen...   \n",
            "\n",
            "                                     Author-Written:  \\\n",
            "0  3AM: An Ambiguity-Aware Multi-Modal Machine Tr...   \n",
            "1  ABLE: Agency-BeLiefs Embedding to Address Ster...   \n",
            "2  A Challenge Dataset and Effective Models for C...   \n",
            "3  AcnEmpathize: A Dataset for Understanding Empa...   \n",
            "4  Adaptive Reinforcement Tuning Language Models ...   \n",
            "\n",
            "                                             Catchy:  \\\n",
            "0  3AM: Enhancing Ambiguity-Aware Multimodal Mach...   \n",
            "1  ABLE Model: Addressing Bias in NLP Through Awa...   \n",
            "2  MT-CSD: Enhancing Conversational Stance Detect...   \n",
            "3  AcnEmpathize: Domain-Specific Empathy Detectio...   \n",
            "4  Enhancing Sentence Representations with Adapti...   \n",
            "\n",
            "                                            Rhyming:  \\\n",
            "0  Visual Clarity for Translational Verity: Intro...   \n",
            "1  Data to Combat the Hate Debate: ABLEâ€™s Role in...   \n",
            "2  Multi-Turn Data for Stance to Relay: Advancing...   \n",
            "3  Empathy Unveiled in Acne's Tale: AcnEmpathize ...   \n",
            "4  Data to Elevate: Contrastive Learning's New Fa...   \n",
            "\n",
            "                                             Funny:   \\\n",
            "0   Lost in Translation: When Images Speak Ambiguity   \n",
            "1   Bias Be Gone: The Awkward Truth About NLP Models   \n",
            "2  Stance Wars: The Quest for Accurate Social Med...   \n",
            "3  Acne and Empathy: Spotting Support in Skin Dee...   \n",
            "4  Teaching Tiny Models to Play Hardball with Sen...   \n",
            "\n",
            "                                      Comprehensive:  \\\n",
            "0  3AM: Enhancing Multimodal Machine Translation ...   \n",
            "1  ABLE: A Novel Approach to Encoding Stereotypic...   \n",
            "2  MT-CSD: A Multi-Turn Dataset and GLAN for Enha...   \n",
            "3  AcnEmpathize: Capturing Domain-Specific Empath...   \n",
            "4  Adaptive Reinforcement Tuning for Enhanced Sen...   \n",
            "\n",
            "                                              Plain:  \n",
            "0  3AM: Ambiguity-Aware Multimodal Machine Transl...  \n",
            "1  ABLE Model: Addressing Stereotypical Biases in...  \n",
            "2  Multi-Turn Conversation Stance Detection Datas...  \n",
            "3  AcnEmpathize: Domain-Specific Empathy Detectio...  \n",
            "4  Adaptive Reinforcement Tuning for Effective Se...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-2798255d91d6>:8: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  data_final = data_final.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking predicted results csv files total rows.\n",
        "import pandas as pd\n",
        "data_final = pd.read_csv(\"LREC-Coling-Style-Title.csv\")\n",
        "print(len(data_final))"
      ],
      "metadata": {
        "id": "jxwasxOUOSlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4ff9bc2-f719-4c02-e19e-349f13a9dde2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets evaluate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1lIRmQPqdVW",
        "outputId": "b4442dfa-6fb9-4e6c-d04c-59da299734bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset  # For loading datasets\n",
        "from evaluate import load as load_metric  # For loading metrics like ROUGE, METEOR, etc.\n"
      ],
      "metadata": {
        "id": "_0f-oaYqrgl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_final.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZcZAW2MwQDf",
        "outputId": "33bb6e33-6d95-406b-d993-ea37a51e0048"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Abstract', 'Author-Written:', 'Catchy:', 'Rhyming:', 'Funny: ',\n",
            "       'Comprehensive:', 'Plain:'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from bert_score import BERTScorer\n",
        "rouge = load_metric(\"rouge\")\n",
        "meteor = load_metric(\"meteor\")\n",
        "l=len(data_final)\n",
        "with open('Comprehensive-score-Rouge-Meteor-BERT.csv', mode='w') as csv_file:\n",
        "  fieldnames = ['Rouge1-F1','Rouge2-F1','RougeL-F1', 'RougeLSum-F1','METEOR_Final','BERT-F1']\n",
        "  writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "  writer.writeheader()\n",
        "\n",
        "  #variables to calculate sum of BERT score\n",
        "  sum_Rouge1_F1=0\n",
        "  sum_Rouge2_F1=0\n",
        "  sum_RougeL_F1=0\n",
        "  sum_RougeLSum_F1=0\n",
        "  sum_METEOR_F1=0\n",
        "  sum_BERT_F1=0\n",
        "  for i in tqdm(range(len(data_final))):\n",
        "    Rouge1 = rouge.compute(predictions=[data_final[\"Comprehensive:\"][i]], references=[data_final[\"Author-Written:\"][i]], rouge_types=[\"rouge1\"])[\"rouge1\"]\n",
        "    Rouge2 = rouge.compute(predictions=[data_final[\"Comprehensive:\"][i]], references=[data_final[\"Author-Written:\"][i]], rouge_types=[\"rouge2\"])[\"rouge2\"]\n",
        "    RougeL = rouge.compute(predictions=[data_final[\"Comprehensive:\"][i]], references=[data_final[\"Author-Written:\"][i]], rouge_types=[\"rougeL\"])[\"rougeL\"]\n",
        "    RougeLSum = rouge.compute(predictions=[data_final[\"Comprehensive:\"][i]], references=[data_final[\"Author-Written:\"][i]], rouge_types=[\"rougeLsum\"])[\"rougeLsum\"]\n",
        "    #Rouge1= rouge.compute(predictions=[data_final[\"Catchy:\"][i]], references=[data_final[\"Author-Written:\"][i]], rouge_types=[\"rouge1\"])[\"rouge1\"].mid\n",
        "    #Rouge2= rouge.compute(predictions=[data_final[\"Catchy:\"][i]], references=[data_final[\"Author-Written:\"][i]], rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
        "    #RougeL= rouge.compute(predictions=[data_final[\"Catchy:\"][i]], references=[data_final[\"Author-Written:\"][i]], rouge_types=[\"rougeL\"])[\"rougeL\"].mid\n",
        "    #RougeLSum= rouge.compute(predictions=[data_final[\"Catchy:\"][i]], references=[data_final[\"Author-Written:\"][i]], rouge_types=[\"rougeLsum\"])[\"rougeLsum\"].mid\n",
        "    METEOR=meteor.compute(predictions=[data_final[\"Comprehensive:\"][i]], references=[data_final[\"Author-Written:\"][i]])['meteor']\n",
        "    scorer = BERTScorer(lang=\"en\")\n",
        "    #data_final[\"Predicted_Title\"].fillna(\"\", inplace=True)\n",
        "    #data_final[\"Silver_Title\"].fillna(\"\", inplace=True)\n",
        "\n",
        "    BERT_P,BERT_R,BERT_F1 = scorer.score([data_final[\"Comprehensive:\"][i]],[data_final[\"Author-Written:\"][i]])#, lang=\"en\", verbose=False)\n",
        "    BERT_F1_scalar = BERT_F1.mean().item()\n",
        "    writer.writerow({'Rouge1-F1':round(Rouge1,4),'Rouge2-F1':round((Rouge2),4),'RougeL-F1':round((RougeL),4),'RougeLSum-F1':round((RougeLSum),4),'METEOR_Final':round(METEOR,4),'BERT-F1': \"{:.4f}\".format(BERT_F1_scalar)})\n",
        "    sum_Rouge1_F1=sum_Rouge1_F1+round((Rouge1),4)\n",
        "    sum_Rouge2_F1=sum_Rouge2_F1+round((Rouge2),4)\n",
        "    sum_RougeL_F1=sum_RougeL_F1+round((RougeL),4)\n",
        "    sum_RougeLSum_F1=sum_RougeLSum_F1+round((RougeLSum),4)\n",
        "    sum_METEOR_F1=sum_METEOR_F1+round(METEOR,4)\n",
        "    sum_BERT_F1=sum_BERT_F1+round(BERT_F1_scalar,4)\n",
        "\n",
        "  Avg_Rouge1_F1=sum_Rouge1_F1/l\n",
        "  Avg_Rouge2_F1=sum_Rouge2_F1/l\n",
        "  Avg_RougeL_F1=sum_RougeL_F1/l\n",
        "  Avg_RougeLSum_F1=sum_RougeLSum_F1/l\n",
        "  Avg_METEOR_F1=sum_METEOR_F1/l\n",
        "  Avg_BERT_F1=sum_BERT_F1/l\n",
        "  print(round(Avg_Rouge1_F1,4))\n",
        "  print(round(Avg_Rouge2_F1,4))\n",
        "  print(round(Avg_RougeL_F1,4))\n",
        "  print(round(Avg_RougeLSum_F1,4))\n",
        "  print(round(Avg_METEOR_F1,4))\n",
        "  print(round(Avg_BERT_F1,4))\n",
        "\n",
        "  #save in a text file\n",
        "with open(\"Comprehensive-score-Rouge-Meteor-BERT.txt\",\"w\") as f:\n",
        "  f.write(\"Avg_Rouge1score:\" + str(Avg_Rouge1_F1) +'\\n')\n",
        "  f.write(\"Avg_Rouge2score:\" + str(Avg_Rouge2_F1) +'\\n')\n",
        "  f.write(\"Avg_RougeLscore:\" + str(Avg_RougeL_F1) +'\\n')\n",
        "  f.write(\"Avg_RougeLSumscore:\" + str(Avg_RougeLSum_F1) +'\\n')\n",
        "  f.write(\"Avg_Meteorscore:\" + str(Avg_METEOR_F1) +'\\n')\n",
        "  f.write(\"Avg_Bertscore:\" + str(Avg_BERT_F1) +'\\n')"
      ],
      "metadata": {
        "id": "YW1C1r7MZHPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import numpy as np\n",
        "import torch\n",
        "import string\n",
        "import os\n",
        "from pyemd import emd, emd_with_flow\n",
        "from torch import nn\n",
        "from math import log\n",
        "from itertools import chain\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "from multiprocessing import Pool\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "#device = 'cuda'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "if os.environ.get('MOVERSCORE_MODEL'):\n",
        "    model_name = os.environ.get('MOVERSCORE_MODEL')\n",
        "else:\n",
        "    model_name = 'distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
        "model = AutoModel.from_pretrained(model_name, output_hidden_states=True, output_attentions=True)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "def truncate(tokens):\n",
        "    if len(tokens) > tokenizer.model_max_length - 2:\n",
        "        tokens = tokens[0:(tokenizer.model_max_length - 2)]\n",
        "    return tokens\n",
        "\n",
        "def process(a):\n",
        "    a = [\"[CLS]\"]+truncate(tokenizer.tokenize(a))+[\"[SEP]\"]\n",
        "    a = tokenizer.convert_tokens_to_ids(a)\n",
        "    return set(a)\n",
        "\n",
        "\n",
        "def get_idf_dict(arr, nthreads=4):\n",
        "    idf_count = Counter()\n",
        "    num_docs = len(arr)\n",
        "\n",
        "    process_partial = partial(process)\n",
        "\n",
        "    with Pool(nthreads) as p:\n",
        "        idf_count.update(chain.from_iterable(p.map(process_partial, arr)))\n",
        "\n",
        "    idf_dict = defaultdict(lambda : log((num_docs+1)/(1)))\n",
        "    idf_dict.update({idx:log((num_docs+1)/(c+1)) for (idx, c) in idf_count.items()})\n",
        "    return idf_dict\n",
        "\n",
        "def padding(arr, pad_token, dtype=torch.long):\n",
        "    lens = torch.LongTensor([len(a) for a in arr])\n",
        "    max_len = lens.max().item()\n",
        "    padded = torch.ones(len(arr), max_len, dtype=dtype) * pad_token\n",
        "    mask = torch.zeros(len(arr), max_len, dtype=torch.long)\n",
        "    for i, a in enumerate(arr):\n",
        "        padded[i, :lens[i]] = torch.tensor(a, dtype=dtype)\n",
        "        mask[i, :lens[i]] = 1\n",
        "    return padded, lens, mask\n",
        "\n",
        "def bert_encode(model, x, attention_mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        result = model(x, attention_mask = attention_mask)\n",
        "    if model_name == 'distilbert-base-uncased':\n",
        "        return result[1]\n",
        "    else:\n",
        "        return result[2]\n",
        "\n",
        "#with open('stopwords.txt', 'r', encoding='utf-8') as f:\n",
        "#    stop_words = set(f.read().strip().split(' '))\n",
        "\n",
        "def collate_idf(arr, tokenize, numericalize, idf_dict,\n",
        "                pad=\"[PAD]\",device='cuda'):\n",
        "\n",
        "    tokens = [[\"[CLS]\"]+truncate(tokenize(a))+[\"[SEP]\"] for a in arr]\n",
        "    arr = [numericalize(a) for a in tokens]\n",
        "\n",
        "    idf_weights = [[idf_dict[i] for i in a] for a in arr]\n",
        "\n",
        "    pad_token = numericalize([pad])[0]\n",
        "\n",
        "    padded, lens, mask = padding(arr, pad_token, dtype=torch.long)\n",
        "    padded_idf, _, _ = padding(idf_weights, pad_token, dtype=torch.float)\n",
        "    padded = padded.to(device=device)\n",
        "    mask = mask.to(device=device)\n",
        "    lens = lens.to(device=device)\n",
        "\n",
        "    return padded, padded_idf, lens, mask, tokens\n",
        "\n",
        "def get_bert_embedding(all_sens, model, tokenizer, idf_dict,\n",
        "                       batch_size=-1,device='cuda'):\n",
        "\n",
        "    padded_sens, padded_idf, lens, mask, tokens = collate_idf(all_sens,\n",
        "                                                      tokenizer.tokenize, tokenizer.convert_tokens_to_ids,\n",
        "                                                      idf_dict,device=device)\n",
        "\n",
        "    if batch_size == -1: batch_size = len(all_sens)\n",
        "\n",
        "    embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(all_sens), batch_size):\n",
        "            batch_embedding = bert_encode(model, padded_sens[i:i+batch_size],\n",
        "                                          attention_mask=mask[i:i+batch_size])\n",
        "            batch_embedding = torch.stack(batch_embedding)\n",
        "            embeddings.append(batch_embedding)\n",
        "            del batch_embedding\n",
        "\n",
        "    total_embedding = torch.cat(embeddings, dim=-3)\n",
        "    return total_embedding, lens, mask, padded_idf, tokens\n",
        "\n",
        "def _safe_divide(numerator, denominator):\n",
        "    return numerator / (denominator + 1e-30)\n",
        "\n",
        "def batched_cdist_l2(x1, x2):\n",
        "    x1_norm = x1.pow(2).sum(dim=-1, keepdim=True)\n",
        "    x2_norm = x2.pow(2).sum(dim=-1, keepdim=True)\n",
        "    res = torch.baddbmm(\n",
        "        x2_norm.transpose(-2, -1),\n",
        "        x1,\n",
        "        x2.transpose(-2, -1),\n",
        "        alpha=-2\n",
        "    ).add_(x1_norm).clamp_min_(1e-30).sqrt_()\n",
        "    return res\n",
        "\n",
        "def word_mover_score(refs, hyps, idf_dict_ref, idf_dict_hyp, stop_words=[], n_gram=1, remove_subwords = True, batch_size=256,device='cuda'):\n",
        "    preds = []\n",
        "    for batch_start in range(0, len(refs), batch_size):\n",
        "        batch_refs = refs[batch_start:batch_start+batch_size]\n",
        "        batch_hyps = hyps[batch_start:batch_start+batch_size]\n",
        "\n",
        "        ref_embedding, ref_lens, ref_masks, ref_idf, ref_tokens = get_bert_embedding(batch_refs, model, tokenizer, idf_dict_ref,device=device)\n",
        "        hyp_embedding, hyp_lens, hyp_masks, hyp_idf, hyp_tokens = get_bert_embedding(batch_hyps, model, tokenizer, idf_dict_hyp,device=device)\n",
        "\n",
        "        ref_embedding = ref_embedding[-1]\n",
        "        hyp_embedding = hyp_embedding[-1]\n",
        "\n",
        "        batch_size = len(ref_tokens)\n",
        "        for i in range(batch_size):\n",
        "            ref_ids = [k for k, w in enumerate(ref_tokens[i])\n",
        "                                if w in stop_words or '##' in w\n",
        "                                or w in set(string.punctuation)]\n",
        "            hyp_ids = [k for k, w in enumerate(hyp_tokens[i])\n",
        "                                if w in stop_words or '##' in w\n",
        "                                or w in set(string.punctuation)]\n",
        "\n",
        "            ref_embedding[i, ref_ids,:] = 0\n",
        "            hyp_embedding[i, hyp_ids,:] = 0\n",
        "\n",
        "            ref_idf[i, ref_ids] = 0\n",
        "            hyp_idf[i, hyp_ids] = 0\n",
        "\n",
        "        raw = torch.cat([ref_embedding, hyp_embedding], 1)\n",
        "\n",
        "        raw.div_(torch.norm(raw, dim=-1).unsqueeze(-1) + 1e-30)\n",
        "\n",
        "        distance_matrix = batched_cdist_l2(raw, raw).double().cpu().numpy()\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            c1 = np.zeros(raw.shape[1], dtype=np.float)\n",
        "            c2 = np.zeros(raw.shape[1], dtype=np.float)\n",
        "            c1[:len(ref_idf[i])] = ref_idf[i]\n",
        "            c2[len(ref_idf[i]):] = hyp_idf[i]\n",
        "\n",
        "            c1 = _safe_divide(c1, np.sum(c1))\n",
        "            c2 = _safe_divide(c2, np.sum(c2))\n",
        "\n",
        "            dst = distance_matrix[i]\n",
        "            _, flow = emd_with_flow(c1, c2, dst)\n",
        "            flow = np.array(flow, dtype=np.float32)\n",
        "            score = 1./(1. + np.sum(flow * dst))#1 - np.sum(flow * dst)\n",
        "            preds.append(score)\n",
        "\n",
        "    return preds\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_example(is_flow, reference, translation, device='cuda'):\n",
        "\n",
        "    idf_dict_ref = defaultdict(lambda: 1.)\n",
        "    idf_dict_hyp = defaultdict(lambda: 1.)\n",
        "\n",
        "    ref_embedding, ref_lens, ref_masks, ref_idf, ref_tokens = get_bert_embedding([reference], model, tokenizer, idf_dict_ref,device=device)\n",
        "    hyp_embedding, hyp_lens, hyp_masks, hyp_idf, hyp_tokens = get_bert_embedding([translation], model, tokenizer, idf_dict_hyp,device=device)\n",
        "\n",
        "    ref_embedding = ref_embedding[-1]\n",
        "    hyp_embedding = hyp_embedding[-1]\n",
        "\n",
        "    raw = torch.cat([ref_embedding, hyp_embedding], 1)\n",
        "    raw.div_(torch.norm(raw, dim=-1).unsqueeze(-1) + 1e-30)\n",
        "\n",
        "    distance_matrix = batched_cdist_l2(raw, raw)\n",
        "    masks = torch.cat([ref_masks, hyp_masks], 1)\n",
        "    masks = torch.einsum('bi,bj->bij', (masks, masks))\n",
        "    distance_matrix = masks * distance_matrix\n",
        "\n",
        "\n",
        "    i = 0\n",
        "    c1 = np.zeros(raw.shape[1], dtype=np.float)\n",
        "    c2 = np.zeros(raw.shape[1], dtype=np.float)\n",
        "    c1[:len(ref_idf[i])] = ref_idf[i]\n",
        "    c2[len(ref_idf[i]):] = hyp_idf[i]\n",
        "\n",
        "    c1 = _safe_divide(c1, np.sum(c1))\n",
        "    c2 = _safe_divide(c2, np.sum(c2))\n",
        "\n",
        "    dst = distance_matrix[i].double().cpu().numpy()\n",
        "\n",
        "    if is_flow:\n",
        "        _, flow = emd_with_flow(c1, c2, dst)\n",
        "        new_flow = np.array(flow, dtype=np.float32)\n",
        "        res = new_flow[:len(ref_tokens[i]), len(ref_idf[i]): (len(ref_idf[i])+len(hyp_tokens[i]))]\n",
        "    else:\n",
        "        res = 1./(1. + dst[:len(ref_tokens[i]), len(ref_idf[i]): (len(ref_idf[i])+len(hyp_tokens[i]))])\n",
        "\n",
        "    r_tokens = ref_tokens[i]\n",
        "    h_tokens = hyp_tokens[i]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(len(r_tokens)*0.8, len(h_tokens)*0.8))\n",
        "    im = ax.imshow(res, cmap='Blues')\n",
        "\n",
        "    ax.set_xticks(np.arange(len(h_tokens)))\n",
        "    ax.set_yticks(np.arange(len(r_tokens)))\n",
        "\n",
        "    ax.set_xticklabels(h_tokens, fontsize=10)\n",
        "    ax.set_yticklabels(r_tokens, fontsize=10)\n",
        "    plt.xlabel(\"System Translation\", fontsize=14)\n",
        "    plt.ylabel(\"Human Reference\", fontsize=14)\n",
        "    plt.title(\"Flow Matrix\", fontsize=14)\n",
        "\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "#    for i in range(len(r_tokens)):\n",
        "#        for j in range(len(h_tokens)):\n",
        "#            text = ax.text(j, i, '{:.2f}'.format(res[i, j].item()),\n",
        "#                           ha=\"center\", va=\"center\", color=\"k\" if res[i, j].item() < 0.6 else \"w\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "IUUeNiO_lh0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_mover_score(refs, hyps, idf_dict_ref, idf_dict_hyp, stop_words=[], n_gram=1, remove_subwords = True, batch_size=256,device='cuda'):\n",
        "    preds = []\n",
        "    for batch_start in range(0, len(refs), batch_size):\n",
        "        batch_refs = refs[batch_start:batch_start+batch_size]\n",
        "        batch_hyps = hyps[batch_start:batch_start+batch_size]\n",
        "\n",
        "        ref_embedding, ref_lens, ref_masks, ref_idf, ref_tokens = get_bert_embedding(batch_refs, model, tokenizer, idf_dict_ref,device=device)\n",
        "        hyp_embedding, hyp_lens, hyp_masks, hyp_idf, hyp_tokens = get_bert_embedding(batch_hyps, model, tokenizer, idf_dict_hyp,device=device)\n",
        "\n",
        "        ref_embedding = ref_embedding[-1]\n",
        "        hyp_embedding = hyp_embedding[-1]\n",
        "\n",
        "        batch_size = len(ref_tokens)\n",
        "        for i in range(batch_size):\n",
        "            ref_ids = [k for k, w in enumerate(ref_tokens[i])\n",
        "                                if w in stop_words or '##' in w\n",
        "                                or w in set(string.punctuation)]\n",
        "            hyp_ids = [k for k, w in enumerate(hyp_tokens[i])\n",
        "                                if w in stop_words or '##' in w\n",
        "                                or w in set(string.punctuation)]\n",
        "\n",
        "            ref_embedding[i, ref_ids,:] = 0\n",
        "            hyp_embedding[i, hyp_ids,:] = 0\n",
        "\n",
        "            ref_idf[i, ref_ids] = 0\n",
        "            hyp_idf[i, hyp_ids] = 0\n",
        "        raw = torch.cat"
      ],
      "metadata": {
        "id": "k3MhZQw5o2kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Specify the device as the first GPU\n",
        "device = torch.device('cuda:0')\n",
        "\n",
        "# Check if CUDA is available, otherwise fall back to CPU\n",
        "if not torch.cuda.is_available():\n",
        "    device = torch.device('cpu')\n",
        "    print(\"CUDA is not available. Using CPU instead.\")\n",
        "else:\n",
        "    print(\"Using CUDA on GPU 0.\")\n",
        "\n",
        "# You can now move tensors or models to the chosen device\n",
        "tensor = torch.tensor([1.0, 2.0, 3.0], device=device)\n",
        "model = torch.nn.Linear(3, 1).to(device)\n",
        "\n",
        "# Example usage\n",
        "output = model(tensor)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVi8edEixVbD",
        "outputId": "6af0a7af-80f8-4099-8928-2814ab8fdfee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA on GPU 0.\n",
            "tensor([1.0085], device='cuda:0', grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "-yuQ-c17xwUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Union, Iterable\n",
        "from itertools import zip_longest\n",
        "#import sacrebleu\n",
        "from moverscore_v2 import word_mover_score\n",
        "from moverscore import get_idf_dict, word_mover_score\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "def sentence_score(hypothesis: str, references: List[str], trace=0):\n",
        "\n",
        "    idf_dict_hyp = defaultdict(lambda: 1.)\n",
        "    idf_dict_ref = defaultdict(lambda: 1.)\n",
        "\n",
        "    hypothesis = [hypothesis] * len(references)\n",
        "\n",
        "    sentence_score = 0\n",
        "\n",
        "    scores = word_mover_score(references, hypothesis, idf_dict_ref, idf_dict_hyp, stop_words=[], n_gram=1, remove_subwords=False)\n",
        "\n",
        "    sentence_score = np.mean(scores)\n",
        "\n",
        "    if trace > 0:\n",
        "        print(hypothesis, references, sentence_score)\n",
        "\n",
        "    return sentence_score\n",
        "\n",
        "def corpus_score(sys_stream: List[str],\n",
        "                     ref_streams:Union[str, List[Iterable[str]]], trace=0):\n",
        "\n",
        "    if isinstance(sys_stream, str):\n",
        "        sys_stream = [sys_stream]\n",
        "\n",
        "    if isinstance(ref_streams, str):\n",
        "        ref_streams = [[ref_streams]]\n",
        "\n",
        "    fhs = [sys_stream] + ref_streams\n",
        "\n",
        "    corpus_score = 0\n",
        "    for lines in zip_longest(*fhs):\n",
        "        if None in lines:\n",
        "            raise EOFError(\"Source and reference streams have different lengths!\")\n",
        "\n",
        "        hypo, *refs = lines\n",
        "        corpus_score += sentence_score(hypo, refs, trace=0)\n",
        "\n",
        "    corpus_score /= len(sys_stream)\n",
        "\n",
        "    return corpus_score\n",
        "\n",
        "def test_corpus_score():\n",
        "\n",
        "    refs = [['The dog bit the man.', 'It was not unexpected.', 'The man bit him first.'],\n",
        "            ['The dog had bit the man.', 'No one was surprised.', 'The man had bitten the dog.']]\n",
        "    sys = ['The dog bit the man.', \"It wasn't surprising.\", 'The man had just bitten him.']\n",
        "\n",
        "\n",
        "    mover = corpus_score(sys, refs)\n",
        "\n",
        "    #print(\"corpus score\", mover)\n",
        "\n",
        "def test_sentence_score():\n",
        "\n",
        "    refs = ['The dog bit the man.', 'The dog had bit the man.']\n",
        "    sys = 'The dog bit the man.'\n",
        "\n",
        "    #bleu = sacrebleu.sentence_bleu(sys, refs)\n",
        "    mover = sentence_score(sys, refs)\n",
        "\n",
        "\n",
        "    #print(\"sentence score\", mover)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    test_sentence_score()\n",
        "\n",
        "    test_corpus_score()"
      ],
      "metadata": {
        "id": "jfsOoaBebvu8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "491da68c-3ac4-434f-c40b-df5b12874355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
            "  warnings.warn(\n",
            "GroupViT models are not usable since `tensorflow_probability` can't be loaded. It seems you have `tensorflow_probability` installed with the wrong tensorflow version.Please try to reinstall it following the instructions here: https://github.com/tensorflow/probability.\n",
            "TAPAS models are not usable since `tensorflow_probability` can't be loaded. It seems you have `tensorflow_probability` installed with the wrong tensorflow version. Please try to reinstall it following the instructions here: https://github.com/tensorflow/probability.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n",
            "Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_attentions\": true,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n",
            "Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_pretrained_bert/modeling.py:603: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(weights_path, map_location='cpu')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "l = len(data_final)\n",
        "\n",
        "with open('Comprehensive-moverScore.csv', mode='w') as csv_file:\n",
        "    fieldnames = ['moverscore', 'corpus_moverscore']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "\n",
        "    moversum_score = 0\n",
        "    corpus_score_mover_sum = 0\n",
        "\n",
        "    for i in tqdm(range(l)):\n",
        "        sample_row = data_final.iloc[i]\n",
        "        #Filename = sample_row['FileName']\n",
        "\n",
        "        predictions = data_final[\"Comprehensive:\"][i]\n",
        "        references = data_final[\"Author-Written:\"][i]\n",
        "\n",
        "        # Check for NaN or empty strings\n",
        "        if pd.isna(predictions):\n",
        "            predictions = \" \"\n",
        "        if pd.isna(references):\n",
        "            references = \" \"\n",
        "\n",
        "        #print(\"Predictions:\", predictions)\n",
        "        #print(\"References:\", references)\n",
        "        mover = sentence_score(predictions, [references])\n",
        "        corpus_score_mover = corpus_score(predictions, references)\n",
        "        moverscores = round(mover, 4)\n",
        "        corpus_score_mover = round(corpus_score_mover, 4)\n",
        "        writer.writerow({'moverscore': moverscores, 'corpus_moverscore': corpus_score_mover})\n",
        "        moversum_score += moverscores\n",
        "        corpus_score_mover_sum += corpus_score_mover\n",
        "    Avg_moversum_score = moversum_score / l\n",
        "    Avg_corpus_score_mover = corpus_score_mover_sum / l\n",
        "    print(\"\\n\")\n",
        "    print(\"Avg_moversum_score\", round(Avg_moversum_score, 4))\n",
        "    print(\"Avg_corpus_score_mover\", round(Avg_corpus_score_mover, 4))\n",
        "# Save in a text file\n",
        "with open(\"Comprehensive-moverScore.txt\", \"w\") as f:\n",
        "    f.write(\"Avg_meteorsum_score:\" + str(Avg_moversum_score) + '\\n')\n",
        "    f.write(\"Avg_corpus_score_mover:\" + str(Avg_corpus_score_mover) + '\\n')"
      ],
      "metadata": {
        "id": "CA1vSZ826XiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgCou7X29_Mr",
        "outputId": "e2620a80-0aa7-4efc-c2ff-9d66f178d2a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Style-Title-generation-ICADL/LREC-Coling'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Style-Title-generation-ICADL/CSPubSum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zZW8LaR-B0U",
        "outputId": "656564dd-6cde-4fe7-8668-0de35da900b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Style-Title-generation-ICADL/CSPubSum\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_final.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WN-khkv49AjN",
        "outputId": "febd4022-7b31-4eec-b7ba-ccb009876436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Abstract', 'Author-Written:', 'Catchy:', 'Rhyming:', 'Funny: ',\n",
            "       'Comprehensive:', 'Plain:'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SciBERTScore\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "from bert_score import BERTScorer\n",
        "\n",
        "# Assume data_final is a DataFrame or a dictionary-like object with appropriate columns\n",
        "#data_final = ...\n",
        "\n",
        "l = len(data_final)\n",
        "with open('Funny-SciBERTScore.csv', mode='w') as csv_file:\n",
        "    fieldnames = ['Title', 'Predicted_Title','BERT-F1']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "\n",
        "    # Variable to calculate sum of BERT score\n",
        "    sum_BERT_F1 = 0\n",
        "\n",
        "    # Initialize the SciBERT scorer\n",
        "    scorer = BERTScorer(model_type=\"allenai/scibert_scivocab_uncased\", lang=\"en\")\n",
        "\n",
        "    for i in tqdm(range(len(data_final))):\n",
        "        # Compute BERT score using SciBERT\n",
        "        data_final[\"Funny: \"].fillna(\"\", inplace=True)\n",
        "        BERT_P, BERT_R, BERT_F1 = scorer.score([data_final[\"Funny: \"][i]], [data_final[\"Author-Written:\"][i]])\n",
        "        BERT_F1_scalar = BERT_F1.mean().item()\n",
        "\n",
        "        # Write scores to CSV\n",
        "        writer.writerow({'Title': data_final[\"Author-Written:\"][i],'Predicted_Title': data_final[\"Funny: \"][i], 'BERT-F1': \"{:.4f}\".format(BERT_F1_scalar)})\n",
        "\n",
        "        # Sum scores for averaging\n",
        "        sum_BERT_F1 += BERT_F1_scalar\n",
        "\n",
        "    # Calculate average\n",
        "    Avg_BERT_F1 = sum_BERT_F1 / l\n",
        "\n",
        "    # Print average\n",
        "    print(round(Avg_BERT_F1, 4))\n",
        "\n",
        "    # Save average to a text file\n",
        "    with open(\"Funny-SciBERTScore.txt\", \"w\") as f:\n",
        "        f.write(\"Avg_Bertscore: \" + str(round(Avg_BERT_F1, 4)) + '\\n')\n"
      ],
      "metadata": {
        "id": "Jmhi5djg8QDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import runtime\n",
        "#runtime.unassign()"
      ],
      "metadata": {
        "id": "o4Vjn9lRjTCJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}